{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dec8b98-0672-4344-b00e-fe7717be1bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           A  B          C  D                E\n",
      "0  -0.545774  0 2000-01-01  y        d jkl d d\n",
      "1  -0.468674  0 2000-01-02  x     d ef ghi abc\n",
      "2   1.767960  0 1999-12-31  v      jkl ef d ef\n",
      "3  -0.118771  1 2000-01-01  y    d jkl abc ghi\n",
      "4   0.630196  0 1999-12-31  w          d d d d\n",
      "..       ... ..        ... ..              ...\n",
      "95 -1.182318 -1 2000-01-01  v  jkl jkl ghi ghi\n",
      "96  0.562761  0 2000-01-01  v      d abc ef ef\n",
      "97 -0.797270  0 2000-01-01  w       ef ghi d d\n",
      "98  0.502741  0 1999-12-31  y    abc ghi d jkl\n",
      "99  2.056356  0 1999-12-30  w      ef d ghi ef\n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import make_regression\n",
    "from datetime import datetime\n",
    "\n",
    "x, y = make_regression(n_samples = 100,n_features = 5,n_targets = 1, random_state = 1)\n",
    "dfx = pd.DataFrame(x, columns=['A','B','C','D','E'])\n",
    "dfy = pd.DataFrame(y, columns=['label'])\n",
    "\n",
    "# Create an integer column, a datetime column, a categorical column and a string column to demonstrate how they are processed.\n",
    "dfx['B'] = (dfx['B']).astype(int)\n",
    "dfx['C'] = datetime(2000,1,1) + pd.to_timedelta(dfx['C'].astype(int), unit='D')\n",
    "dfx['D'] = pd.cut(dfx['D'] * 10, [-np.inf,-5,0,5,np.inf],labels=['v','w','x','y'])\n",
    "dfx['E'] = pd.Series(list(' '.join(random.choice([\"abc\", \"d\", \"ef\", \"ghi\", \"jkl\"]) for i in range(4)) for j in range(100)))\n",
    "dataset=TabularDataset(dfx)\n",
    "print(dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f411b5-4661-469b-9927-0ea6ec2d159f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>C</th>\n",
       "      <th>C.year</th>\n",
       "      <th>C.month</th>\n",
       "      <th>C.day</th>\n",
       "      <th>C.dayofweek</th>\n",
       "      <th>E.char_count</th>\n",
       "      <th>E.symbol_ratio.</th>\n",
       "      <th>__nlp__.abc</th>\n",
       "      <th>__nlp__.ef</th>\n",
       "      <th>__nlp__.ghi</th>\n",
       "      <th>__nlp__.jkl</th>\n",
       "      <th>__nlp__._total_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.545774</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.468674</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>946771200000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.767960</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946598400000000000</td>\n",
       "      <td>1999</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.118771</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.630196</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946598400000000000</td>\n",
       "      <td>1999</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-1.182318</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.562761</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.797270</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.502741</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946598400000000000</td>\n",
       "      <td>1999</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2.056356</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946512000000000000</td>\n",
       "      <td>1999</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           A  B  D    E                   C  C.year  C.month  C.day  \\\n",
       "0  -0.545774  0  3  NaN  946684800000000000    2000        1      1   \n",
       "1  -0.468674  0  2    1  946771200000000000    2000        1      2   \n",
       "2   1.767960  0  0  NaN  946598400000000000    1999       12     31   \n",
       "3  -0.118771  1  3  NaN  946684800000000000    2000        1      1   \n",
       "4   0.630196  0  1  NaN  946598400000000000    1999       12     31   \n",
       "..       ... .. ..  ...                 ...     ...      ...    ...   \n",
       "95 -1.182318 -1  0  NaN  946684800000000000    2000        1      1   \n",
       "96  0.562761  0  0  NaN  946684800000000000    2000        1      1   \n",
       "97 -0.797270  0  1  NaN  946684800000000000    2000        1      1   \n",
       "98  0.502741  0  3  NaN  946598400000000000    1999       12     31   \n",
       "99  2.056356  0  1  NaN  946512000000000000    1999       12     30   \n",
       "\n",
       "    C.dayofweek  E.char_count  E.symbol_ratio.   __nlp__.abc  __nlp__.ef  \\\n",
       "0             5             2                 6            0           0   \n",
       "1             6             5                 3            1           1   \n",
       "2             4             4                 4            0           2   \n",
       "3             5             6                 2            1           0   \n",
       "4             4             0                 8            0           0   \n",
       "..          ...           ...               ...          ...         ...   \n",
       "95            5             8                 0            0           0   \n",
       "96            5             4                 4            1           2   \n",
       "97            5             3                 5            0           1   \n",
       "98            4             6                 2            1           0   \n",
       "99            3             4                 4            0           2   \n",
       "\n",
       "    __nlp__.ghi  __nlp__.jkl  __nlp__._total_  \n",
       "0             0            1                1  \n",
       "1             1            0                3  \n",
       "2             0            1                2  \n",
       "3             1            1                3  \n",
       "4             0            0                0  \n",
       "..          ...          ...              ...  \n",
       "95            2            2                2  \n",
       "96            0            0                2  \n",
       "97            1            0                2  \n",
       "98            1            1                3  \n",
       "99            1            0                2  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
    "auto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()\n",
    "auto_ml_pipeline_feature_generator.fit_transform(X=dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba046e5-166b-4c6d-929e-2ceaa31f5738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240903_123652\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.8.19\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #40~22.04.3-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 30 17:30:19 UTC 2\n",
      "CPU Count:          32\n",
      "Memory Avail:       56.77 GB / 62.51 GB (90.8%)\n",
      "Disk Space Avail:   681.15 GB / 843.94 GB (80.7%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240903_123652\"\n",
      "Train Data Rows:    100\n",
      "Train Data Columns: 5\n",
      "Label Column:       label\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (186.98105511749836, -267.99365510467214, 9.38193, 71.29287)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "AutoMLPipelineFeatureGenerator is already fit, so the training data will be processed via .transform() instead of .fit_transform().\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', [])     : 1 | ['D']\n",
      "\t\t('datetime', [])     : 1 | ['C']\n",
      "\t\t('float', [])        : 1 | ['A']\n",
      "\t\t('int', [])          : 1 | ['B']\n",
      "\t\t('object', ['text']) : 1 | ['E']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    : 1 | ['D']\n",
      "\t\t('category', ['text_as_category'])  : 1 | ['E']\n",
      "\t\t('float', [])                       : 1 | ['A']\n",
      "\t\t('int', [])                         : 1 | ['B']\n",
      "\t\t('int', ['binned', 'text_special']) : 2 | ['E.char_count', 'E.symbol_ratio. ']\n",
      "\t\t('int', ['datetime_as_int'])        : 5 | ['C', 'C.year', 'C.month', 'C.day', 'C.dayofweek']\n",
      "\t\t('int', ['text_ngram'])             : 5 | ['__nlp__.abc', '__nlp__.ef', '__nlp__.ghi', '__nlp__.jkl', '__nlp__._total_']\n",
      "Data preprocessing and feature engineering runtime = 0.02s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 80, Val Rows: 20\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': {},\n",
      "}\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM ...\n",
      "\t-60.7413\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM': 1.0}\n",
      "\t-60.7413\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 0.17s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2724.5 rows/s (20 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240903_123652\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x7314ae5f0dc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([dfx, dfy], axis=1)\n",
    "predictor = TabularPredictor(label='label')\n",
    "predictor.fit(df, hyperparameters={'GBM' : {}}, feature_generator=auto_ml_pipeline_feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c1b0495-1988-43eb-bf03-e32fb56ab087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(set(dfx['B'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4edc89a0-a82d-40cf-8cdf-fa2d52179570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    58102.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['E']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 4\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', [])     : 2 | ['B', 'D']\n",
      "\t\t('datetime', [])     : 1 | ['C']\n",
      "\t\t('float', [])        : 1 | ['A']\n",
      "\t\t('object', ['text']) : 1 | ['E']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    : 2 | ['B', 'D']\n",
      "\t\t('category', ['text_as_category'])  : 1 | ['E']\n",
      "\t\t('float', [])                       : 1 | ['A']\n",
      "\t\t('int', ['binned', 'text_special']) : 2 | ['E.char_count', 'E.symbol_ratio. ']\n",
      "\t\t('int', ['datetime_as_int'])        : 5 | ['C', 'C.year', 'C.month', 'C.day', 'C.dayofweek']\n",
      "\t\t('int', ['text_ngram'])             : 5 | ['__nlp__.abc', '__nlp__.ef', '__nlp__.ghi', '__nlp__.jkl', '__nlp__._total_']\n",
      "\t0.1s = Fit runtime\n",
      "\t5 features in original data used to generate 16 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>C</th>\n",
       "      <th>C.year</th>\n",
       "      <th>C.month</th>\n",
       "      <th>C.day</th>\n",
       "      <th>C.dayofweek</th>\n",
       "      <th>E.char_count</th>\n",
       "      <th>E.symbol_ratio.</th>\n",
       "      <th>__nlp__.abc</th>\n",
       "      <th>__nlp__.ef</th>\n",
       "      <th>__nlp__.ghi</th>\n",
       "      <th>__nlp__.jkl</th>\n",
       "      <th>__nlp__._total_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.545774</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.468674</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>946771200000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.767960</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946598400000000000</td>\n",
       "      <td>1999</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.118771</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.630196</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946598400000000000</td>\n",
       "      <td>1999</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-1.182318</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.562761</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.797270</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.502741</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946598400000000000</td>\n",
       "      <td>1999</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2.056356</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946512000000000000</td>\n",
       "      <td>1999</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           A  B  D    E                   C  C.year  C.month  C.day  \\\n",
       "0  -0.545774  1  3  NaN  946684800000000000    2000        1      1   \n",
       "1  -0.468674  1  2    1  946771200000000000    2000        1      2   \n",
       "2   1.767960  1  0  NaN  946598400000000000    1999       12     31   \n",
       "3  -0.118771  2  3  NaN  946684800000000000    2000        1      1   \n",
       "4   0.630196  1  1  NaN  946598400000000000    1999       12     31   \n",
       "..       ... .. ..  ...                 ...     ...      ...    ...   \n",
       "95 -1.182318  0  0  NaN  946684800000000000    2000        1      1   \n",
       "96  0.562761  1  0  NaN  946684800000000000    2000        1      1   \n",
       "97 -0.797270  1  1  NaN  946684800000000000    2000        1      1   \n",
       "98  0.502741  1  3  NaN  946598400000000000    1999       12     31   \n",
       "99  2.056356  1  1  NaN  946512000000000000    1999       12     30   \n",
       "\n",
       "    C.dayofweek  E.char_count  E.symbol_ratio.   __nlp__.abc  __nlp__.ef  \\\n",
       "0             5             2                 6            0           0   \n",
       "1             6             5                 3            1           1   \n",
       "2             4             4                 4            0           2   \n",
       "3             5             6                 2            1           0   \n",
       "4             4             0                 8            0           0   \n",
       "..          ...           ...               ...          ...         ...   \n",
       "95            5             8                 0            0           0   \n",
       "96            5             4                 4            1           2   \n",
       "97            5             3                 5            0           1   \n",
       "98            4             6                 2            1           0   \n",
       "99            3             4                 4            0           2   \n",
       "\n",
       "    __nlp__.ghi  __nlp__.jkl  __nlp__._total_  \n",
       "0             0            1                1  \n",
       "1             1            0                3  \n",
       "2             0            1                2  \n",
       "3             1            1                3  \n",
       "4             0            0                0  \n",
       "..          ...          ...              ...  \n",
       "95            2            2                2  \n",
       "96            0            0                2  \n",
       "97            1            0                2  \n",
       "98            1            1                3  \n",
       "99            1            0                2  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx[\"B\"] = dfx[\"B\"].astype(\"category\")\n",
    "auto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()\n",
    "auto_ml_pipeline_feature_generator.fit_transform(X=dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753645f8-9490-400c-b75d-9cf9f9503a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.468674</td>\n",
       "      <td>0</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>x</td>\n",
       "      <td>d ef ghi abc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.767960</td>\n",
       "      <td>0</td>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>v</td>\n",
       "      <td>jkl ef d ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.118771</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>y</td>\n",
       "      <td>d jkl abc ghi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.630196</td>\n",
       "      <td>0</td>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>w</td>\n",
       "      <td>d d d d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A    B          C    D              E\n",
       "0       NaN  NaN        NaT  NaN            NaN\n",
       "1 -0.468674    0 2000-01-02    x   d ef ghi abc\n",
       "2  1.767960    0 1999-12-31    v    jkl ef d ef\n",
       "3 -0.118771    1 2000-01-01    y  d jkl abc ghi\n",
       "4  0.630196    0 1999-12-31    w        d d d d"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx.iloc[0] = np.nan\n",
    "dfx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20fc7943-0546-4b75-af7f-e2bfb28061a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    58104.43 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['E']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 4\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', [])     : 2 | ['B', 'D']\n",
      "\t\t('datetime', [])     : 1 | ['C']\n",
      "\t\t('float', [])        : 1 | ['A']\n",
      "\t\t('object', ['text']) : 1 | ['E']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    : 2 | ['B', 'D']\n",
      "\t\t('category', ['text_as_category'])  : 1 | ['E']\n",
      "\t\t('float', [])                       : 1 | ['A']\n",
      "\t\t('int', ['binned', 'text_special']) : 3 | ['E.char_count', 'E.word_count', 'E.symbol_ratio. ']\n",
      "\t\t('int', ['datetime_as_int'])        : 5 | ['C', 'C.year', 'C.month', 'C.day', 'C.dayofweek']\n",
      "\t\t('int', ['text_ngram'])             : 5 | ['__nlp__.abc', '__nlp__.ef', '__nlp__.ghi', '__nlp__.jkl', '__nlp__._total_']\n",
      "\t0.1s = Fit runtime\n",
      "\t5 features in original data used to generate 17 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>C</th>\n",
       "      <th>C.year</th>\n",
       "      <th>C.month</th>\n",
       "      <th>C.day</th>\n",
       "      <th>C.dayofweek</th>\n",
       "      <th>E.char_count</th>\n",
       "      <th>E.word_count</th>\n",
       "      <th>E.symbol_ratio.</th>\n",
       "      <th>__nlp__.abc</th>\n",
       "      <th>__nlp__.ef</th>\n",
       "      <th>__nlp__.ghi</th>\n",
       "      <th>__nlp__.jkl</th>\n",
       "      <th>__nlp__._total_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946687418181818240</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.468674</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>946771200000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.767960</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946598400000000000</td>\n",
       "      <td>1999</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.118771</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.630196</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946598400000000000</td>\n",
       "      <td>1999</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-1.182318</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.562761</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.797270</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946684800000000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.502741</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946598400000000000</td>\n",
       "      <td>1999</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2.056356</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946512000000000000</td>\n",
       "      <td>1999</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           A    B    D    E                   C  C.year  C.month  C.day  \\\n",
       "0        NaN  NaN  NaN  NaN  946687418181818240    2000        1      1   \n",
       "1  -0.468674    1    2    1  946771200000000000    2000        1      2   \n",
       "2   1.767960    1    0  NaN  946598400000000000    1999       12     31   \n",
       "3  -0.118771    2    3  NaN  946684800000000000    2000        1      1   \n",
       "4   0.630196    1    1  NaN  946598400000000000    1999       12     31   \n",
       "..       ...  ...  ...  ...                 ...     ...      ...    ...   \n",
       "95 -1.182318    0    0  NaN  946684800000000000    2000        1      1   \n",
       "96  0.562761    1    0  NaN  946684800000000000    2000        1      1   \n",
       "97 -0.797270    1    1  NaN  946684800000000000    2000        1      1   \n",
       "98  0.502741    1    3  NaN  946598400000000000    1999       12     31   \n",
       "99  2.056356    1    1  NaN  946512000000000000    1999       12     30   \n",
       "\n",
       "    C.dayofweek  E.char_count  E.word_count  E.symbol_ratio.   __nlp__.abc  \\\n",
       "0             5             0             0                 0            0   \n",
       "1             6             6             1                 4            1   \n",
       "2             4             5             1                 5            0   \n",
       "3             5             7             1                 3            1   \n",
       "4             4             1             1                 9            0   \n",
       "..          ...           ...           ...               ...          ...   \n",
       "95            5             9             1                 1            0   \n",
       "96            5             5             1                 5            1   \n",
       "97            5             4             1                 6            0   \n",
       "98            4             7             1                 3            1   \n",
       "99            3             5             1                 5            0   \n",
       "\n",
       "    __nlp__.ef  __nlp__.ghi  __nlp__.jkl  __nlp__._total_  \n",
       "0            0            0            0                0  \n",
       "1            1            1            0                3  \n",
       "2            2            0            1                2  \n",
       "3            0            1            1                3  \n",
       "4            0            0            0                0  \n",
       "..         ...          ...          ...              ...  \n",
       "95           0            2            2                2  \n",
       "96           2            0            0                2  \n",
       "97           1            1            0                2  \n",
       "98           0            1            1                3  \n",
       "99           2            1            0                2  \n",
       "\n",
       "[100 rows x 17 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()\n",
    "auto_ml_pipeline_feature_generator.fit_transform(X=dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e3df101-174d-4249-8cab-5a8613a0163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: /home/michael/Documenti/Milinda_Githubproject/Weibo and Twitter/twitter/Feartures_twitter.csv | Columns = 20 / 20 | Rows = 363 -> 363\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241105_121021\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.8.19\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #48~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Oct  7 11:24:13 UTC 2\n",
      "CPU Count:          32\n",
      "Memory Avail:       48.63 GB / 62.51 GB (77.8%)\n",
      "Disk Space Avail:   637.23 GB / 843.94 GB (75.5%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241105_121021\"\n",
      "Train Data Rows:    290\n",
      "Train Data Columns: 19\n",
      "Label Column:       Label\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    49798.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['Dissimilarity']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['SecondMoment', 'AverageGrayLevel']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['SecondMoment', 'AverageGrayLevel']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 15 | ['Mean', 'Variance', 'Homogeneity', 'Contrast', 'Entropy', ...]\n",
      "\t\t('int', [])   :  1 | ['ImageNum']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 15 | ['Mean', 'Variance', 'Homogeneity', 'Contrast', 'Entropy', ...]\n",
      "\t\t('int', [])   :  1 | ['ImageNum']\n",
      "\t0.0s = Fit runtime\n",
      "\t16 features in original data used to generate 16 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 232, Val Rows: 58\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.569\t = Validation score   (accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.6207\t = Validation score   (accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.19s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9483\t = Validation score   (accuracy)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3.5s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1014.4 rows/s (58 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241105_121021\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data: {'accuracy': 0.9726027397260274, 'balanced_accuracy': 0.9725975975975976, 'mcc': 0.9451951951951952, 'roc_auc': 0.9992492492492492, 'f1': 0.9722222222222222, 'precision': 0.9722222222222222, 'recall': 0.9722222222222222}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These features in provided data are not utilized by the predictor and will be ignored: ['Dissimilarity', 'SecondMoment', 'AverageGrayLevel']\n",
      "Computing feature importance via permutation shuffling for 16 features using 73 rows with 5 shuffle sets...\n",
      "\t3.34s\t= Expected runtime (0.67s per shuffle set)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGluon infers problem type is: binary\n",
      "AutoGluon identified the following types of features:\n",
      "('float', []) : 15 | ['Mean', 'Variance', 'Homogeneity', 'Contrast', 'Entropy', ...]\n",
      "('int', [])   :  1 | ['ImageNum']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.34s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "                  importance    stddev   p_value  n  p99_high   p99_low\n",
      "ImageNum            0.482192  0.096086  0.000180  5  0.680034  0.284350\n",
      "Variance            0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "StdDeviation        0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "Kurtosis            0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "OverlappingRatio    0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "Entropy            -0.002740  0.006126  0.813050  5  0.009874 -0.015354\n",
      "Histogram          -0.002740  0.006126  0.813050  5  0.009874 -0.015354\n",
      "Mean               -0.005479  0.007503  0.911096  5  0.009969 -0.020928\n",
      "Homogeneity        -0.005479  0.007503  0.911096  5  0.009969 -0.020928\n",
      "Contrast           -0.005479  0.007503  0.911096  5  0.009969 -0.020928\n",
      "Correlation        -0.005479  0.007503  0.911096  5  0.009969 -0.020928\n",
      "Roughness          -0.005479  0.007503  0.911096  5  0.009969 -0.020928\n",
      "Energy             -0.008219  0.007503  0.964758  5  0.007230 -0.023668\n",
      "Skewness           -0.008219  0.007503  0.964758  5  0.007230 -0.023668\n",
      "Perimeter          -0.010959  0.006126  0.991935  5  0.001655 -0.023573\n",
      "Area               -0.010959  0.011461  0.950350  5  0.012640 -0.034557\n",
      "Best model: WeightedEnsemble_L2\n",
      "Best Model: WeightedEnsemble_L2\n",
      "Accuracy: 0.973\n",
      "Precision: 0.972\n",
      "Recall: 0.972\n",
      "F1 Score: 0.972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_157515/2826505778.py:49: DeprecationWarning: `get_model_best` has been deprecated and will be removed in version 1.2. Please use `model_best` instead. This will raise an error in the future!\n",
      "  best_model = predictor.get_model_best()\n",
      "/tmp/ipykernel_157515/2826505778.py:53: DeprecationWarning: `get_model_best` has been deprecated and will be removed in version 1.2. Please use `model_best` instead. This will raise an error in the future!\n",
      "  best_model = predictor.get_model_best()\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load your dataset\n",
    "data = TabularDataset('/home/michael/Documenti/Milinda_Githubproject/Weibo and Twitter/twitter/Feartures_twitter.csv')\n",
    "\n",
    "# Define the label (target column)\n",
    "label = 'Label'  # Assuming the label column is named 'label'\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%) sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data[label])\n",
    "\n",
    "# Display the first few rows of training data\n",
    "train_data.head()\n",
    "\n",
    "# Check the unique classes in the label\n",
    "print(f\"Unique classes: {list(train_data[label].unique())}\")\n",
    "\n",
    "# Create and train the predictor on the training data\n",
    "predictor = TabularPredictor(label=label).fit(train_data)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = predictor.predict(test_data)\n",
    "y_pred.head()  # Predictions\n",
    "\n",
    "# Evaluate the model performance on the test data\n",
    "performance = predictor.evaluate(test_data)\n",
    "print(\"Performance on test data:\", performance)\n",
    "\n",
    "# Display the leaderboard on the test data\n",
    "predictor.leaderboard(test_data)\n",
    "\n",
    "# Print the inferred problem type and feature metadata\n",
    "print(\"AutoGluon infers problem type is:\", predictor.problem_type)\n",
    "print(\"AutoGluon identified the following types of features:\")\n",
    "print(predictor.feature_metadata)\n",
    "\n",
    "# Transform test data features (optional step to see how data is transformed)\n",
    "test_data_transform = predictor.transform_features(test_data)\n",
    "test_data_transform.head()\n",
    "\n",
    "# Calculate and display feature importance\n",
    "feature_importance = predictor.feature_importance(test_data)\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Display the best model\n",
    "best_model = predictor.get_model_best()\n",
    "print(\"Best model:\", best_model)\n",
    "\n",
    "# Get the best model\n",
    "best_model = predictor.get_model_best()\n",
    "\n",
    "# Predict on the test data using the best model\n",
    "y_pred = predictor.predict(test_data, model=best_model)\n",
    "y_true = test_data[label]\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, and F1 Score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec061ba3-e865-47c1-9e26-e8e3e23b66bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241118_162239\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.8.19\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #48~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Oct  7 11:24:13 UTC 2\n",
      "CPU Count:          32\n",
      "Memory Avail:       46.62 GB / 62.51 GB (74.6%)\n",
      "Disk Space Avail:   636.06 GB / 843.94 GB (75.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241118_162239\"\n",
      "Train Data Rows:    222\n",
      "Train Data Columns: 19\n",
      "Label Column:       Outlabel\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    47733.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.05 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['image_id', 'averageGrayLevels']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', [])  : 1 | ['averageGrayLevels']\n",
      "\t\t('object', []) : 1 | ['image_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['meanValues', 'varianceValues', 'homogeneityValues', 'contrastValues', 'entropyValues', ...]\n",
      "\t\t('int', [])   :  4 | ['dissimilarityValues', 'histogramValues', 'areaValues', 'perimeterValues']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 13 | ['meanValues', 'varianceValues', 'homogeneityValues', 'contrastValues', 'entropyValues', ...]\n",
      "\t\t('int', [])       :  3 | ['histogramValues', 'areaValues', 'perimeterValues']\n",
      "\t\t('int', ['bool']) :  1 | ['dissimilarityValues']\n",
      "\t0.0s = Fit runtime\n",
      "\t17 features in original data used to generate 17 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 177, Val Rows: 45\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.7556\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.7333\t = Validation score   (accuracy)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   image_id  Outlabel  meanValues  varianceValues  \\\n",
      "83   Fake_77-Webpage_27.jpg         0  118.697687     3245.324168   \n",
      "47   Fake_61-Webpage_11.jpg         0  103.598472     5421.708795   \n",
      "258   Real_78-Webpage_1.jpg         1   63.179424     3100.910636   \n",
      "79   Fake_77-Webpage_20.jpg         0  127.575787     4021.653401   \n",
      "85    Fake_77-Webpage_4.jpg         0   91.036326     4165.866185   \n",
      "\n",
      "     homogeneityValues  contrastValues  dissimilarityValues  entropyValues  \\\n",
      "83            0.338918      223.135914                    1       7.543059   \n",
      "47            0.304017      440.482806                    1       7.618395   \n",
      "258           0.436176      334.709043                    2       5.996937   \n",
      "79            0.255009      694.443603                    1       7.303061   \n",
      "85            0.257787      577.481615                    1       7.650724   \n",
      "\n",
      "     secondMomentValues  correlationValues  energyValues  stdDeviation  \\\n",
      "83          3245.268137           0.965533      0.000908     56.967747   \n",
      "47          5421.558192           0.959539      0.000865     73.632254   \n",
      "258         3100.906386           0.946072      0.012049     55.685821   \n",
      "79          4021.004224           0.913123      0.000576     63.416507   \n",
      "85          4165.794261           0.930756      0.000241     64.543522   \n",
      "\n",
      "     skewnessValue  kurtosisValue  histogramValues  areaValues  \\\n",
      "83        0.584937       2.813157            -1259       57920   \n",
      "47        0.235235       1.804634            -2596       36000   \n",
      "258       1.130056       4.823757            53096      729640   \n",
      "79        0.691402       2.132151            -2317        6195   \n",
      "85        0.515691       2.125984            -7897       57920   \n",
      "\n",
      "     perimeterValues  roughnessValues  averageGrayLevels  overlappingRatios  \n",
      "83              4695         5.503214         118.697687           8.137217  \n",
      "47              2445         3.635153         103.598472          11.387625  \n",
      "258            15195         5.018125          63.179424          11.333659  \n",
      "79               546         1.956893         127.575787          21.104307  \n",
      "85              4178         4.897216          91.036326           9.598955  \n",
      "Unique classes: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 1: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.52s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1570.8 rows/s (45 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241118_162239\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data: {'accuracy': 0.9464285714285714, 'balanced_accuracy': 0.9464285714285714, 'mcc': 0.8980265101338745, 'roc_auc': 0.9700255102040817, 'f1': 0.9433962264150945, 'precision': 1.0, 'recall': 0.8928571428571429}\n",
      "Leaderboard:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These features in provided data are not utilized by the predictor and will be ignored: ['image_id', 'averageGrayLevels']\n",
      "Computing feature importance via permutation shuffling for 17 features using 56 rows with 5 shuffle sets...\n",
      "\t2.74s\t= Expected runtime (0.55s per shuffle set)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  model  score_test  score_val eval_metric  pred_time_test  \\\n",
      "0              CatBoost    0.946429   1.000000    accuracy        0.001876   \n",
      "1            LightGBMXT    0.946429   1.000000    accuracy        0.013855   \n",
      "2        NeuralNetTorch    0.946429   1.000000    accuracy        0.013866   \n",
      "3              LightGBM    0.946429   1.000000    accuracy        0.015988   \n",
      "4         LightGBMLarge    0.946429   1.000000    accuracy        0.017001   \n",
      "5               XGBoost    0.946429   1.000000    accuracy        0.025340   \n",
      "6        ExtraTreesEntr    0.946429   1.000000    accuracy        0.026603   \n",
      "7      RandomForestEntr    0.946429   1.000000    accuracy        0.026919   \n",
      "8      RandomForestGini    0.946429   1.000000    accuracy        0.030469   \n",
      "9        ExtraTreesGini    0.946429   1.000000    accuracy        0.034390   \n",
      "10  WeightedEnsemble_L2    0.946429   1.000000    accuracy        0.035437   \n",
      "11      NeuralNetFastAI    0.892857   1.000000    accuracy        0.007535   \n",
      "12       KNeighborsDist    0.785714   0.733333    accuracy        0.006878   \n",
      "13       KNeighborsUnif    0.767857   0.755556    accuracy        0.001223   \n",
      "\n",
      "    pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
      "0        0.000679  0.208994                 0.001876                0.000679   \n",
      "1        0.005892  0.161008                 0.013855                0.005892   \n",
      "2        0.005052  0.523215                 0.013866                0.005052   \n",
      "3        0.007535  0.108751                 0.015988                0.007535   \n",
      "4        0.004057  0.208830                 0.017001                0.004057   \n",
      "5        0.004364  0.180473                 0.025340                0.004364   \n",
      "6        0.026276  0.290250                 0.026603                0.026276   \n",
      "7        0.037716  0.305156                 0.026919                0.037716   \n",
      "8        0.063421  0.402769                 0.030469                0.063421   \n",
      "9        0.028309  0.309120                 0.034390                0.028309   \n",
      "10       0.028647  0.349213                 0.001046                0.000338   \n",
      "11       0.006089  1.308343                 0.007535                0.006089   \n",
      "12       0.045702  0.033947                 0.006878                0.045702   \n",
      "13       0.037223  0.007926                 0.001223                0.037223   \n",
      "\n",
      "    fit_time_marginal  stack_level  can_infer  fit_order  \n",
      "0            0.208994            1       True          7  \n",
      "1            0.161008            1       True          3  \n",
      "2            0.523215            1       True         12  \n",
      "3            0.108751            1       True          4  \n",
      "4            0.208830            1       True         13  \n",
      "5            0.180473            1       True         11  \n",
      "6            0.290250            1       True          9  \n",
      "7            0.305156            1       True          6  \n",
      "8            0.402769            1       True          5  \n",
      "9            0.309120            1       True          8  \n",
      "10           0.040092            2       True         14  \n",
      "11           1.308343            1       True         10  \n",
      "12           0.033947            1       True          2  \n",
      "13           0.007926            1       True          1  \n",
      "AutoGluon infers problem type is: binary\n",
      "AutoGluon identified the following types of features:\n",
      "('float', [])     : 13 | ['meanValues', 'varianceValues', 'homogeneityValues', 'contrastValues', 'entropyValues', ...]\n",
      "('int', [])       :  3 | ['histogramValues', 'areaValues', 'perimeterValues']\n",
      "('int', ['bool']) :  1 | ['dissimilarityValues']\n",
      "     meanValues  varianceValues  homogeneityValues  contrastValues  \\\n",
      "116  104.309582     2960.129973           0.347273      354.959948   \n",
      "138   92.085872     2287.411925           0.210591      129.101736   \n",
      "251   99.726604     5107.442818           0.322653      450.877847   \n",
      "107   87.606042     4196.822566           0.341361      144.788333   \n",
      "131  111.507990     3751.992958           0.439732       76.899788   \n",
      "\n",
      "     dissimilarityValues  entropyValues  secondMomentValues  \\\n",
      "116                    0       7.326328         2960.078865   \n",
      "138                    0       7.367030         2287.405117   \n",
      "251                    1       7.668143         5107.427869   \n",
      "107                    0       7.595984         4196.810628   \n",
      "131                    0       7.653764         3751.978635   \n",
      "\n",
      "     correlationValues  energyValues  stdDeviation  skewnessValue  \\\n",
      "116           0.940106      0.001715     54.407076      -0.184744   \n",
      "138           0.971778      0.000194     47.826895       0.014353   \n",
      "251           0.955772      0.000490     71.466375       0.520806   \n",
      "107           0.982751      0.001571     64.782888       0.296844   \n",
      "131           0.989738      0.000663     61.253514       0.422666   \n",
      "\n",
      "     kurtosisValue  histogramValues  areaValues  perimeterValues  \\\n",
      "116       1.736574           -12803       57920             5483   \n",
      "138       1.687883             1332      336000            16921   \n",
      "251       2.027251             2876      341640            23697   \n",
      "107       2.075148           -35068      351550            18647   \n",
      "131       2.587696           -10161      261960             8851   \n",
      "\n",
      "     roughnessValues  overlappingRatios  \n",
      "116         6.426863           1.488747  \n",
      "138         8.234770           5.163022  \n",
      "251        11.436788           6.384588  \n",
      "107         8.871774           8.087804  \n",
      "131         4.878316           6.221649  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.34s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n",
      "/tmp/ipykernel_1141699/1360064914.py:51: DeprecationWarning: `get_model_best` has been deprecated and will be removed in version 1.2. Please use `model_best` instead. This will raise an error in the future!\n",
      "  best_model = predictor.get_model_best()\n",
      "/tmp/ipykernel_1141699/1360064914.py:55: DeprecationWarning: `get_model_best` has been deprecated and will be removed in version 1.2. Please use `model_best` instead. This will raise an error in the future!\n",
      "  best_model = predictor.get_model_best()\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241118_162244\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.8.19\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #48~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Oct  7 11:24:13 UTC 2\n",
      "CPU Count:          32\n",
      "Memory Avail:       46.04 GB / 62.51 GB (73.6%)\n",
      "Disk Space Avail:   636.06 GB / 843.94 GB (75.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241118_162244\"\n",
      "Train Data Rows:    278\n",
      "Train Data Columns: 19\n",
      "Label Column:       Outlabel\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    47141.85 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['image_id', 'averageGrayLevels']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', [])  : 1 | ['averageGrayLevels']\n",
      "\t\t('object', []) : 1 | ['image_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['meanValues', 'varianceValues', 'homogeneityValues', 'contrastValues', 'entropyValues', ...]\n",
      "\t\t('int', [])   :  4 | ['dissimilarityValues', 'histogramValues', 'areaValues', 'perimeterValues']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 13 | ['meanValues', 'varianceValues', 'homogeneityValues', 'contrastValues', 'entropyValues', ...]\n",
      "\t\t('int', [])       :  3 | ['histogramValues', 'areaValues', 'perimeterValues']\n",
      "\t\t('int', ['bool']) :  1 | ['dissimilarityValues']\n",
      "\t0.0s = Fit runtime\n",
      "\t17 features in original data used to generate 17 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 222, Val Rows: 56\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.7679\t = Validation score   (accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.7857\t = Validation score   (accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "                     importance    stddev   p_value  n  p99_high   p99_low\n",
      "dissimilarityValues    0.432143  0.059761  0.000043  5  0.555192  0.309093\n",
      "meanValues             0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "stdDeviation           0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "roughnessValues        0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "perimeterValues        0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "areaValues             0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "histogramValues        0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "kurtosisValue          0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "skewnessValue          0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "energyValues           0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "varianceValues         0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "correlationValues      0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "secondMomentValues     0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "entropyValues          0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "contrastValues         0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "homogeneityValues      0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "overlappingRatios      0.000000  0.000000  0.500000  5  0.000000  0.000000\n",
      "Best model: WeightedEnsemble_L2\n",
      "Best Model: WeightedEnsemble_L2\n",
      "Accuracy: 0.946\n",
      "Precision: 0.952\n",
      "Recall: 0.946\n",
      "F1 Score: 0.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9821\t = Validation score   (accuracy)\n",
      "\t0.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9821\t = Validation score   (accuracy)\n",
      "\t0.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9821\t = Validation score   (accuracy)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9821\t = Validation score   (accuracy)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9821\t = Validation score   (accuracy)\n",
      "\t0.19s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9643\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9643\t = Validation score   (accuracy)\n",
      "\t0.36s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 3: early stopping\n",
      "\t0.9821\t = Validation score   (accuracy)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9643\t = Validation score   (accuracy)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'XGBoost': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3.57s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3843.0 rows/s (56 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241118_162244\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with image IDs saved to '/home/michael/Documenti/Milinda_Githubproject/Weibo and Twitter/twitter/image_predicted_label_autoGlu_Bussfeed.csv'\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "data = TabularDataset('/home/michael/Documenti/Milinda_Githubproject/Weibo and Twitter/twitter/Feartures_bussfeed_image.csv')\n",
    "# Drop the original \"image_id(s)\" column and keep only \"image_id\"\n",
    "data = data.drop(columns=['Imnagenum'])\n",
    "# Define the label (target column)\n",
    "label = 'Outlabel'  # Assuming the label column is named 'Outlabel'\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%) sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data[label])\n",
    "\n",
    "# Display the first few rows of training data\n",
    "print(train_data.head())\n",
    "\n",
    "# Check the unique classes in the label\n",
    "print(f\"Unique classes: {list(train_data[label].unique())}\")\n",
    "\n",
    "# Create and train the predictor on the training data\n",
    "predictor = TabularPredictor(label=label).fit(train_data)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = predictor.predict(test_data)\n",
    "\n",
    "# Evaluate the model performance on the test data\n",
    "performance = predictor.evaluate(test_data)\n",
    "print(\"Performance on test data:\", performance)\n",
    "\n",
    "# Display the leaderboard on the test data\n",
    "print(\"Leaderboard:\")\n",
    "print(predictor.leaderboard(test_data))\n",
    "\n",
    "# Print the inferred problem type and feature metadata\n",
    "print(\"AutoGluon infers problem type is:\", predictor.problem_type)\n",
    "print(\"AutoGluon identified the following types of features:\")\n",
    "print(predictor.feature_metadata)\n",
    "\n",
    "# Transform test data features (optional step to see how data is transformed)\n",
    "test_data_transform = predictor.transform_features(test_data)\n",
    "print(test_data_transform.head())\n",
    "\n",
    "# Calculate and display feature importance\n",
    "feature_importance = predictor.feature_importance(test_data)\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Display the best model\n",
    "best_model = predictor.get_model_best()\n",
    "print(\"Best model:\", best_model)\n",
    "\n",
    "# Get the best model\n",
    "best_model = predictor.get_model_best()\n",
    "\n",
    "# Predict on the test data using the best model\n",
    "y_pred = predictor.predict(test_data, model=best_model)\n",
    "y_true = test_data[label]\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, and F1 Score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n",
    "\n",
    "# Predict on the entire dataset using the best model\n",
    "predictor1 = TabularPredictor(label=label).fit(data)\n",
    "y_pred1 = predictor1.predict(data)\n",
    "\n",
    "# Add predictions to the original data\n",
    "data_with_predictions = data.copy()\n",
    "data_with_predictions['predicted_label_image'] = y_pred1\n",
    "\n",
    "# Save to a new CSV file\n",
    "output_path = '/home/michael/Documenti/Milinda_Githubproject/Weibo and Twitter/twitter/image_predicted_label_autoGlu_Bussfeed.csv'\n",
    "data_with_predictions.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Predictions with image IDs saved to '{output_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1371e-5ce8-4020-9088-95eaf752b233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4183e-1d97-4a39-8790-40956937e491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
